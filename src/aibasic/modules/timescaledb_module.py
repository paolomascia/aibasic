"""
TimescaleDB Module for AIbasic

This module provides integration with TimescaleDB, a PostgreSQL extension
optimized for time-series data with fast ingest, complex queries, and automatic partitioning.

Features:
- Hypertable management (time-series optimized tables)
- Continuous aggregates (materialized views that auto-update)
- Data retention policies (automatic data deletion)
- Compression policies (automatic compression of old data)
- Time-bucket queries for efficient aggregation
- Downsampling and rollups
- Gap filling for missing time-series data
- PostgreSQL compatibility (all PostgreSQL features available)

Author: AIbasic Team
Version: 1.0
"""

import threading
from typing import Any, Dict, List, Optional, Union
from datetime import datetime, timedelta
import pandas as pd


class TimescaleDBModule:
    """
    TimescaleDB module for AIbasic programs.

    Provides integration with TimescaleDB for time-series data storage,
    analysis, and efficient querying with automatic partitioning.

    This class implements the singleton pattern to ensure only one instance
    exists per process, with thread-safe initialization.
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        """
        Create or return the singleton instance (thread-safe).
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(
        self,
        host: str = "localhost",
        port: int = 5432,
        database: str = "tsdb",
        user: str = "postgres",
        password: str = "password",
        pool_size: int = 5,
        max_overflow: int = 10,
        **kwargs
    ):
        """
        Initialize the TimescaleDB module.

        Args:
            host: Database host
            port: Database port
            database: Database name
            user: Username
            password: Password
            pool_size: Connection pool size
            max_overflow: Maximum overflow connections
            **kwargs: Additional psycopg2 parameters
        """
        if self._initialized:
            return

        try:
            import psycopg2
            import psycopg2.pool
            import psycopg2.extras
        except ImportError:
            raise ImportError(
                "psycopg2-binary package is required. Install with: pip install psycopg2-binary"
            )

        # Create connection pool
        self.pool = psycopg2.pool.ThreadedConnectionPool(
            pool_size,
            pool_size + max_overflow,
            host=host,
            port=port,
            database=database,
            user=user,
            password=password,
            **kwargs
        )

        # Store psycopg2 for exception handling
        self.psycopg2 = psycopg2

        self._initialized = True

    def get_connection(self):
        """Get a connection from the pool."""
        return self.pool.getconn()

    def release_connection(self, conn):
        """Release a connection back to the pool."""
        self.pool.putconn(conn)

    def execute_query(
        self,
        query: str,
        params: Optional[tuple] = None,
        fetch: bool = True
    ) -> Optional[List[Dict[str, Any]]]:
        """
        Execute a query and return results.

        Args:
            query: SQL query
            params: Query parameters
            fetch: Whether to fetch results

        Returns:
            List of dictionaries (rows) or None
        """
        conn = self.get_connection()
        try:
            with conn.cursor(cursor_factory=self.psycopg2.extras.RealDictCursor) as cur:
                cur.execute(query, params)
                if fetch:
                    return [dict(row) for row in cur.fetchall()]
                conn.commit()
                return None
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            self.release_connection(conn)

    def execute_many(self, query: str, params_list: List[tuple]) -> int:
        """
        Execute query multiple times with different parameters.

        Args:
            query: SQL query
            params_list: List of parameter tuples

        Returns:
            Number of affected rows
        """
        conn = self.get_connection()
        try:
            with conn.cursor() as cur:
                cur.executemany(query, params_list)
                conn.commit()
                return cur.rowcount
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            self.release_connection(conn)

    def query_to_dataframe(self, query: str, params: Optional[tuple] = None) -> pd.DataFrame:
        """
        Execute query and return results as DataFrame.

        Args:
            query: SQL query
            params: Query parameters

        Returns:
            pandas DataFrame
        """
        conn = self.get_connection()
        try:
            df = pd.read_sql_query(query, conn, params=params)
            return df
        finally:
            self.release_connection(conn)

    # ========================================================================
    # Hypertable Management
    # ========================================================================

    def create_hypertable(
        self,
        table_name: str,
        time_column: str = "time",
        chunk_time_interval: str = "1 day",
        if_not_exists: bool = True,
        partitioning_column: Optional[str] = None,
        number_partitions: Optional[int] = None,
        create_default_indexes: bool = True
    ) -> Dict[str, Any]:
        """
        Convert a regular table to a hypertable.

        Args:
            table_name: Name of the table
            time_column: Name of the time column
            chunk_time_interval: Chunk interval (e.g., '1 day', '1 week')
            if_not_exists: Don't error if already a hypertable
            partitioning_column: Additional partitioning column
            number_partitions: Number of space partitions
            create_default_indexes: Create default indexes

        Returns:
            Dictionary with result
        """
        try:
            query = f"""
            SELECT create_hypertable(
                '{table_name}',
                '{time_column}',
                chunk_time_interval => INTERVAL '{chunk_time_interval}',
                if_not_exists => {if_not_exists},
                create_default_indexes => {create_default_indexes}
            """

            if partitioning_column and number_partitions:
                query += f",\n                partitioning_column => '{partitioning_column}'"
                query += f",\n                number_partitions => {number_partitions}"

            query += "\n            );"

            self.execute_query(query, fetch=False)
            return {'success': True, 'table': table_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def drop_hypertable(
        self,
        table_name: str,
        cascade: bool = False
    ) -> Dict[str, Any]:
        """
        Drop a hypertable.

        Args:
            table_name: Name of the hypertable
            cascade: Drop dependent objects

        Returns:
            Dictionary with result
        """
        try:
            cascade_str = "CASCADE" if cascade else ""
            query = f"DROP TABLE {table_name} {cascade_str};"
            self.execute_query(query, fetch=False)
            return {'success': True, 'table': table_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def add_retention_policy(
        self,
        table_name: str,
        drop_after: str,
        if_not_exists: bool = True
    ) -> Dict[str, Any]:
        """
        Add automatic data retention policy.

        Args:
            table_name: Name of the hypertable
            drop_after: Drop data older than this (e.g., '7 days', '3 months')
            if_not_exists: Don't error if policy exists

        Returns:
            Dictionary with result
        """
        try:
            query = f"""
            SELECT add_retention_policy(
                '{table_name}',
                INTERVAL '{drop_after}',
                if_not_exists => {if_not_exists}
            );
            """
            self.execute_query(query, fetch=False)
            return {'success': True, 'table': table_name, 'drop_after': drop_after}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def remove_retention_policy(
        self,
        table_name: str,
        if_exists: bool = True
    ) -> Dict[str, Any]:
        """
        Remove retention policy from hypertable.

        Args:
            table_name: Name of the hypertable
            if_exists: Don't error if policy doesn't exist

        Returns:
            Dictionary with result
        """
        try:
            query = f"""
            SELECT remove_retention_policy('{table_name}', if_exists => {if_exists});
            """
            self.execute_query(query, fetch=False)
            return {'success': True, 'table': table_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def add_compression_policy(
        self,
        table_name: str,
        compress_after: str,
        if_not_exists: bool = True
    ) -> Dict[str, Any]:
        """
        Add automatic compression policy.

        Args:
            table_name: Name of the hypertable
            compress_after: Compress chunks older than this (e.g., '7 days')
            if_not_exists: Don't error if policy exists

        Returns:
            Dictionary with result
        """
        try:
            # First enable compression on the hypertable
            query1 = f"ALTER TABLE {table_name} SET (timescaledb.compress);"
            self.execute_query(query1, fetch=False)

            # Then add the policy
            query2 = f"""
            SELECT add_compression_policy(
                '{table_name}',
                INTERVAL '{compress_after}',
                if_not_exists => {if_not_exists}
            );
            """
            self.execute_query(query2, fetch=False)
            return {'success': True, 'table': table_name, 'compress_after': compress_after}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def remove_compression_policy(
        self,
        table_name: str,
        if_exists: bool = True
    ) -> Dict[str, Any]:
        """
        Remove compression policy from hypertable.

        Args:
            table_name: Name of the hypertable
            if_exists: Don't error if policy doesn't exist

        Returns:
            Dictionary with result
        """
        try:
            query = f"""
            SELECT remove_compression_policy('{table_name}', if_exists => {if_exists});
            """
            self.execute_query(query, fetch=False)
            return {'success': True, 'table': table_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    # ========================================================================
    # Continuous Aggregates
    # ========================================================================

    def create_continuous_aggregate(
        self,
        view_name: str,
        hypertable: str,
        time_column: str,
        bucket_interval: str,
        select_query: str,
        with_data: bool = True
    ) -> Dict[str, Any]:
        """
        Create a continuous aggregate (auto-updating materialized view).

        Args:
            view_name: Name for the continuous aggregate
            hypertable: Source hypertable name
            time_column: Time column name
            bucket_interval: Bucket size (e.g., '1 hour', '1 day')
            select_query: Aggregation query (SELECT columns, AGG() FROM ... GROUP BY ...)
            with_data: Populate with existing data

        Returns:
            Dictionary with result
        """
        try:
            with_data_str = "WITH DATA" if with_data else "WITH NO DATA"

            query = f"""
            CREATE MATERIALIZED VIEW {view_name}
            WITH (timescaledb.continuous) AS
            SELECT time_bucket(INTERVAL '{bucket_interval}', {time_column}) AS bucket,
                   {select_query}
            {with_data_str};
            """
            self.execute_query(query, fetch=False)
            return {'success': True, 'view': view_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def drop_continuous_aggregate(
        self,
        view_name: str,
        cascade: bool = False
    ) -> Dict[str, Any]:
        """
        Drop a continuous aggregate.

        Args:
            view_name: Name of the continuous aggregate
            cascade: Drop dependent objects

        Returns:
            Dictionary with result
        """
        try:
            cascade_str = "CASCADE" if cascade else ""
            query = f"DROP MATERIALIZED VIEW {view_name} {cascade_str};"
            self.execute_query(query, fetch=False)
            return {'success': True, 'view': view_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def refresh_continuous_aggregate(
        self,
        view_name: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Manually refresh a continuous aggregate.

        Args:
            view_name: Name of the continuous aggregate
            start_time: Start time for refresh (optional)
            end_time: End time for refresh (optional)

        Returns:
            Dictionary with result
        """
        try:
            if start_time and end_time:
                query = f"""
                CALL refresh_continuous_aggregate(
                    '{view_name}',
                    '{start_time}'::timestamptz,
                    '{end_time}'::timestamptz
                );
                """
            else:
                query = f"CALL refresh_continuous_aggregate('{view_name}', NULL, NULL);"

            self.execute_query(query, fetch=False)
            return {'success': True, 'view': view_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def add_continuous_aggregate_policy(
        self,
        view_name: str,
        start_offset: str,
        end_offset: str,
        schedule_interval: str
    ) -> Dict[str, Any]:
        """
        Add automatic refresh policy for continuous aggregate.

        Args:
            view_name: Name of the continuous aggregate
            start_offset: How far back to refresh (e.g., '1 month')
            end_offset: How recent to refresh (e.g., '1 hour')
            schedule_interval: How often to refresh (e.g., '1 hour')

        Returns:
            Dictionary with result
        """
        try:
            query = f"""
            SELECT add_continuous_aggregate_policy(
                '{view_name}',
                start_offset => INTERVAL '{start_offset}',
                end_offset => INTERVAL '{end_offset}',
                schedule_interval => INTERVAL '{schedule_interval}'
            );
            """
            self.execute_query(query, fetch=False)
            return {'success': True, 'view': view_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    # ========================================================================
    # Time-Series Query Helpers
    # ========================================================================

    def time_bucket_query(
        self,
        table_name: str,
        time_column: str,
        bucket_interval: str,
        aggregations: Dict[str, str],
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        where_clause: Optional[str] = None,
        order_by: str = "bucket ASC"
    ) -> pd.DataFrame:
        """
        Execute time-bucket aggregation query.

        Args:
            table_name: Name of the hypertable
            time_column: Time column name
            bucket_interval: Bucket size (e.g., '5 minutes', '1 hour')
            aggregations: Dict of column: function (e.g., {'value': 'AVG', 'count': 'COUNT(*)'})
            start_time: Start time filter
            end_time: End time filter
            where_clause: Additional WHERE conditions
            order_by: ORDER BY clause

        Returns:
            pandas DataFrame with results
        """
        # Build aggregation part
        agg_parts = []
        for col, func in aggregations.items():
            if func.upper() == 'COUNT(*)':
                agg_parts.append(f"COUNT(*) AS {col}")
            else:
                agg_parts.append(f"{func}({col}) AS {col}")

        agg_str = ", ".join(agg_parts)

        # Build WHERE clause
        where_parts = []
        if start_time:
            where_parts.append(f"{time_column} >= '{start_time}'")
        if end_time:
            where_parts.append(f"{time_column} < '{end_time}'")
        if where_clause:
            where_parts.append(where_clause)

        where_str = ""
        if where_parts:
            where_str = "WHERE " + " AND ".join(where_parts)

        query = f"""
        SELECT time_bucket(INTERVAL '{bucket_interval}', {time_column}) AS bucket,
               {agg_str}
        FROM {table_name}
        {where_str}
        GROUP BY bucket
        ORDER BY {order_by};
        """

        return self.query_to_dataframe(query)

    def fill_gaps(
        self,
        table_name: str,
        time_column: str,
        bucket_interval: str,
        start_time: str,
        end_time: str,
        columns: List[str],
        fill_method: str = "locf"  # locf, linear, or NULL
    ) -> pd.DataFrame:
        """
        Fill gaps in time-series data.

        Args:
            table_name: Name of the hypertable
            time_column: Time column name
            bucket_interval: Bucket size
            start_time: Start time
            end_time: End time
            columns: Columns to include
            fill_method: Fill method (locf=last observation carried forward, linear, NULL)

        Returns:
            pandas DataFrame with filled gaps
        """
        cols_str = ", ".join(columns)

        if fill_method.lower() == "locf":
            fill_func = "locf"
        elif fill_method.lower() == "linear":
            fill_func = "interpolate"
        else:
            fill_func = "NULL"

        query = f"""
        SELECT
            time_bucket_gapfill(INTERVAL '{bucket_interval}', {time_column}) AS bucket,
            {fill_func}(AVG({cols_str})) AS value
        FROM {table_name}
        WHERE {time_column} >= '{start_time}'
          AND {time_column} < '{end_time}'
        GROUP BY bucket
        ORDER BY bucket;
        """

        return self.query_to_dataframe(query)

    def get_hypertable_stats(self, table_name: str) -> Dict[str, Any]:
        """
        Get statistics about a hypertable.

        Args:
            table_name: Name of the hypertable

        Returns:
            Dictionary with statistics
        """
        try:
            query = f"""
            SELECT *
            FROM timescaledb_information.hypertables
            WHERE hypertable_name = '{table_name}';
            """
            result = self.execute_query(query)

            if result:
                return {'success': True, 'stats': result[0]}
            else:
                return {'success': False, 'error': 'Hypertable not found'}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def get_chunk_info(self, table_name: str) -> List[Dict[str, Any]]:
        """
        Get information about chunks for a hypertable.

        Args:
            table_name: Name of the hypertable

        Returns:
            List of chunk information
        """
        query = f"""
        SELECT *
        FROM timescaledb_information.chunks
        WHERE hypertable_name = '{table_name}'
        ORDER BY range_start DESC;
        """
        return self.execute_query(query)

    def compress_chunk(self, chunk_name: str) -> Dict[str, Any]:
        """
        Manually compress a specific chunk.

        Args:
            chunk_name: Name of the chunk

        Returns:
            Dictionary with result
        """
        try:
            query = f"SELECT compress_chunk('{chunk_name}');"
            self.execute_query(query, fetch=False)
            return {'success': True, 'chunk': chunk_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def decompress_chunk(self, chunk_name: str) -> Dict[str, Any]:
        """
        Manually decompress a specific chunk.

        Args:
            chunk_name: Name of the chunk

        Returns:
            Dictionary with result
        """
        try:
            query = f"SELECT decompress_chunk('{chunk_name}');"
            self.execute_query(query, fetch=False)
            return {'success': True, 'chunk': chunk_name}
        except Exception as e:
            return {'success': False, 'error': str(e)}

    def close(self):
        """Close all connections in the pool."""
        if hasattr(self, 'pool'):
            self.pool.closeall()


# Singleton instance getter
def get_timescaledb_module(**kwargs) -> TimescaleDBModule:
    """
    Get the singleton TimescaleDB module instance.

    Args:
        **kwargs: Configuration parameters

    Returns:
        TimescaleDBModule instance
    """
    return TimescaleDBModule(**kwargs)
